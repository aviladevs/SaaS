# Logstash Configuration for Ãvila DevOps SaaS
# Pipeline for processing and forwarding logs to Elasticsearch

input {
  # Beats input for Filebeat logs
  beats {
    port => 5044
    tags => ["beats"]
  }

  # TCP input for application logs
  tcp {
    port => 5000
    codec => json
    tags => ["tcp"]
  }

  # Syslog input
  syslog {
    port => 5514
    tags => ["syslog"]
  }

  # HTTP input for webhooks
  http {
    port => 8080
    codec => json
    tags => ["http"]
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "parsed"
    }
  }

  # Extract common fields
  if [parsed] {
    mutate {
      add_field => {
        "log_level" => "%{[parsed][level]}"
        "service_name" => "%{[parsed][service]}"
        "environment" => "%{[parsed][environment]}"
      }
    }
  }

  # Parse Django logs
  if [tags] and "django" in [tags] {
    grok {
      match => {
        "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] %{LOGLEVEL:level} %{DATA:logger}: %{GREEDYDATA:log_message}"
      }
    }
  }

  # Parse Nginx access logs
  if [tags] and "nginx" in [tags] and [log_type] == "access" {
    grok {
      match => {
        "message" => '%{IPORHOST:remote_addr} - %{DATA:remote_user} \[%{HTTPDATE:time_local}\] "%{WORD:request_method} %{DATA:request_uri} HTTP/%{NUMBER:http_version}" %{NUMBER:status} %{NUMBER:body_bytes_sent} "%{DATA:http_referer}" "%{DATA:http_user_agent}"'
      }
    }
    
    mutate {
      convert => {
        "status" => "integer"
        "body_bytes_sent" => "integer"
      }
    }
  }

  # Parse Nginx error logs
  if [tags] and "nginx" in [tags] and [log_type] == "error" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] %{NUMBER:pid}#%{NUMBER:tid}: \*%{NUMBER:connection_id} %{GREEDYDATA:log_message}"
      }
    }
  }

  # Add geo-location for IP addresses
  if [remote_addr] {
    geoip {
      source => "remote_addr"
      target => "geoip"
    }
  }

  # Add timestamp
  date {
    match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss"]
    target => "@timestamp"
  }

  # Add common metadata
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "logstash"
      "cluster" => "aviladevops-saas"
    }
  }

  # Remove parsed field if exists
  if [parsed] {
    mutate {
      remove_field => ["parsed"]
    }
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logstash-%{[service_name]:other}-%{+YYYY.MM.dd}"
    template_name => "logstash"
    template_overwrite => true
  }

  # Debug output (only in development)
  # stdout { codec => rubydebug }
}
