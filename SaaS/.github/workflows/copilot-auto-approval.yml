# Auto-Approval Workflow for GitHub Copilot PRs
# Machine Learning-style automatic approval system

name: ü§ñ Copilot PR Auto-Approval

on:
  pull_request:
    types: [opened, synchronize, ready_for_review]
  pull_request_review:
    types: [submitted]

jobs:
  ai-review-and-approve:
    name: AI-Powered Review & Auto-Approval
    runs-on: ubuntu-latest
    if: github.actor == 'copilot-swe-agent' || contains(github.head_ref, 'copilot/')
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: üß† AI Quality Assessment
      id: ai-assessment
      run: |
        echo "ü§ñ Running AI-powered code assessment..."
        
        # Get PR information
        PR_NUMBER="${{ github.event.number }}"
        PR_TITLE="${{ github.event.pull_request.title }}"
        PR_AUTHOR="${{ github.event.pull_request.user.login }}"
        
        # Initialize scoring system
        QUALITY_SCORE=0
        MAX_SCORE=100
        
        echo "üìä Analyzing PR #$PR_NUMBER by $PR_AUTHOR"
        echo "Title: $PR_TITLE"
        
        # Criterion 1: Author is Copilot (30 points)
        if [[ "$PR_AUTHOR" == "copilot-swe-agent" ]]; then
          QUALITY_SCORE=$((QUALITY_SCORE + 30))
          echo "‚úÖ Author is GitHub Copilot (+30 points)"
        fi
        
        # Criterion 2: Branch name pattern (10 points)
        if [[ "${{ github.head_ref }}" =~ ^copilot/ ]]; then
          QUALITY_SCORE=$((QUALITY_SCORE + 10))
          echo "‚úÖ Copilot branch pattern (+10 points)"
        fi
        
        # Criterion 3: PR title quality (20 points)
        if [[ "$PR_TITLE" =~ (Optimize|Implement|Fix|Add|Update|Configure) ]]; then
          QUALITY_SCORE=$((QUALITY_SCORE + 20))
          echo "‚úÖ Quality PR title (+20 points)"
        fi
        
        # Criterion 4: File analysis (40 points max)
        echo "üîç Analyzing changed files..."
        
        # Get list of changed files
        git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.event.pull_request.head.sha }} > changed_files.txt
        
        INFRASTRUCTURE_FILES=0
        DOCUMENTATION_FILES=0
        CONFIG_FILES=0
        POTENTIALLY_DANGEROUS=0
        
        while IFS= read -r file; do
          echo "  üìÅ $file"
          
          # Infrastructure files (+10 points)
          if [[ "$file" =~ (infrastructure/|kubernetes/|docker/|nginx/) ]]; then
            INFRASTRUCTURE_FILES=$((INFRASTRUCTURE_FILES + 1))
          fi
          
          # Documentation files (+5 points)
          if [[ "$file" =~ (\.md$|README|docs/) ]]; then
            DOCUMENTATION_FILES=$((DOCUMENTATION_FILES + 1))
          fi
          
          # Configuration files (+5 points)  
          if [[ "$file" =~ (\.yml$|\.yaml$|\.json$|\.conf$) ]]; then
            CONFIG_FILES=$((CONFIG_FILES + 1))
          fi
          
          # Potentially dangerous files (-20 points each)
          if [[ "$file" =~ (\.env$|secrets|password|key|token) ]]; then
            POTENTIALLY_DANGEROUS=$((POTENTIALLY_DANGEROUS + 1))
          fi
          
        done < changed_files.txt
        
        # Calculate file-based score
        FILE_SCORE=0
        if [[ $INFRASTRUCTURE_FILES -gt 0 ]]; then
          FILE_SCORE=$((FILE_SCORE + 15))
          echo "‚úÖ Infrastructure improvements (+15 points)"
        fi
        
        if [[ $DOCUMENTATION_FILES -gt 0 ]]; then
          FILE_SCORE=$((FILE_SCORE + 10))
          echo "‚úÖ Documentation included (+10 points)"
        fi
        
        if [[ $CONFIG_FILES -gt 0 ]]; then
          FILE_SCORE=$((FILE_SCORE + 15))
          echo "‚úÖ Configuration files (+15 points)"
        fi
        
        if [[ $POTENTIALLY_DANGEROUS -gt 0 ]]; then
          FILE_SCORE=$((FILE_SCORE - (POTENTIALLY_DANGEROUS * 20)))
          echo "‚ö†Ô∏è Potentially dangerous files detected (-$((POTENTIALLY_DANGEROUS * 20)) points)"
        fi
        
        QUALITY_SCORE=$((QUALITY_SCORE + FILE_SCORE))
        
        echo "üìä Final Quality Score: $QUALITY_SCORE/$MAX_SCORE"
        
        # Set outputs
        echo "score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
        echo "max_score=$MAX_SCORE" >> $GITHUB_OUTPUT
        echo "file_count=$(wc -l < changed_files.txt)" >> $GITHUB_OUTPUT
        echo "infrastructure_files=$INFRASTRUCTURE_FILES" >> $GITHUB_OUTPUT
        echo "documentation_files=$DOCUMENTATION_FILES" >> $GITHUB_OUTPUT
        echo "dangerous_files=$POTENTIALLY_DANGEROUS" >> $GITHUB_OUTPUT

    - name: üß™ Automated Testing
      id: automated-tests
      run: |
        echo "üß™ Running automated tests..."
        
        TEST_SCORE=0
        
        # Check for test files
        if find . -name "*test*" -o -name "*spec*" | grep -q .; then
          TEST_SCORE=$((TEST_SCORE + 20))
          echo "‚úÖ Test files found (+20 points)"
        fi
        
        # Check for scripts
        if find . -name "*.sh" -o -name "*.ps1" | grep -q .; then
          TEST_SCORE=$((TEST_SCORE + 15))
          echo "‚úÖ Scripts included (+15 points)"
        fi
        
        # Syntax validation for YAML files
        YAML_VALID=true
        for file in $(find . -name "*.yml" -o -name "*.yaml"); do
          if ! python3 -c "import yaml; yaml.safe_load(open('$file'))" 2>/dev/null; then
            YAML_VALID=false
            echo "‚ùå Invalid YAML: $file"
          fi
        done
        
        if [[ "$YAML_VALID" == "true" ]]; then
          TEST_SCORE=$((TEST_SCORE + 15))
          echo "‚úÖ All YAML files valid (+15 points)"
        fi
        
        echo "test_score=$TEST_SCORE" >> $GITHUB_OUTPUT

    - name: üîí Security Analysis
      id: security-check
      run: |
        echo "üîí Running security analysis..."
        
        SECURITY_SCORE=0
        
        # Check for secrets in code
        if ! grep -r -i "password\|secret\|token\|key" . --include="*.py" --include="*.js" --include="*.ts"; then
          SECURITY_SCORE=$((SECURITY_SCORE + 25))
          echo "‚úÖ No hardcoded secrets detected (+25 points)"
        else
          echo "‚ö†Ô∏è Potential secrets found in code"
        fi
        
        # Check for proper secret handling
        if grep -r "secrets\." . --include="*.yml" --include="*.yaml"; then
          SECURITY_SCORE=$((SECURITY_SCORE + 15))
          echo "‚úÖ Proper secret management detected (+15 points)"
        fi
        
        # Check for security headers in configs
        if grep -r "X-Frame-Options\|X-Content-Type-Options\|Strict-Transport-Security" . --include="*.conf" --include="*.yml"; then
          SECURITY_SCORE=$((SECURITY_SCORE + 10))
          echo "‚úÖ Security headers configured (+10 points)"
        fi
        
        echo "security_score=$SECURITY_SCORE" >> $GITHUB_OUTPUT

    - name: üéØ AI Decision Engine
      id: ai-decision
      run: |
        echo "üéØ AI Decision Engine processing..."
        
        QUALITY_SCORE="${{ steps.ai-assessment.outputs.score }}"
        TEST_SCORE="${{ steps.automated-tests.outputs.test_score }}"
        SECURITY_SCORE="${{ steps.security-check.outputs.security_score }}"
        DANGEROUS_FILES="${{ steps.ai-assessment.outputs.dangerous_files }}"
        
        TOTAL_SCORE=$((QUALITY_SCORE + TEST_SCORE + SECURITY_SCORE))
        
        echo "üìä Scoring Summary:"
        echo "  Quality Score: $QUALITY_SCORE/100"
        echo "  Test Score: $TEST_SCORE/50"
        echo "  Security Score: $SECURITY_SCORE/50"
        echo "  Total Score: $TOTAL_SCORE/200"
        echo "  Dangerous Files: $DANGEROUS_FILES"
        
        # AI Decision Logic
        AUTO_APPROVE=false
        CONFIDENCE_LEVEL="LOW"
        
        # High confidence auto-approval (Score >= 150 and no dangerous files)
        if [[ $TOTAL_SCORE -ge 150 && $DANGEROUS_FILES -eq 0 ]]; then
          AUTO_APPROVE=true
          CONFIDENCE_LEVEL="HIGH"
          echo "ü§ñ AI Decision: AUTO-APPROVE (High Confidence)"
        
        # Medium confidence auto-approval (Score >= 120 and no dangerous files)
        elif [[ $TOTAL_SCORE -ge 120 && $DANGEROUS_FILES -eq 0 ]]; then
          AUTO_APPROVE=true
          CONFIDENCE_LEVEL="MEDIUM"
          echo "ü§ñ AI Decision: AUTO-APPROVE (Medium Confidence)"
        
        # Low score or dangerous files - require manual review
        else
          AUTO_APPROVE=false
          echo "ü§ñ AI Decision: MANUAL REVIEW REQUIRED"
          echo "  Reason: Score too low ($TOTAL_SCORE < 120) or dangerous files detected ($DANGEROUS_FILES)"
        fi
        
        echo "auto_approve=$AUTO_APPROVE" >> $GITHUB_OUTPUT
        echo "confidence=$CONFIDENCE_LEVEL" >> $GITHUB_OUTPUT
        echo "total_score=$TOTAL_SCORE" >> $GITHUB_OUTPUT

    - name: ‚úÖ Auto-Approve High-Quality PRs
      if: steps.ai-decision.outputs.auto_approve == 'true'
      run: |
        echo "üöÄ Auto-approving high-quality PR..."
        
        # Add approval comment with AI reasoning
        gh pr review ${{ github.event.number }} --approve --body "## ü§ñ AI Auto-Approval
        
        **Confidence Level:** ${{ steps.ai-decision.outputs.confidence }}
        **Total Score:** ${{ steps.ai-decision.outputs.total_score }}/200
        
        ### üìä Scoring Breakdown:
        - **Quality Score:** ${{ steps.ai-assessment.outputs.score }}/100
        - **Test Score:** ${{ steps.automated-tests.outputs.test_score }}/50  
        - **Security Score:** ${{ steps.security-check.outputs.security_score }}/50
        
        ### ‚úÖ AI Assessment:
        - Author: GitHub Copilot ‚úÖ
        - Files Changed: ${{ steps.ai-assessment.outputs.file_count }}
        - Infrastructure Files: ${{ steps.ai-assessment.outputs.infrastructure_files }}
        - Documentation: ${{ steps.ai-assessment.outputs.documentation_files }}
        - Security Issues: ${{ steps.ai-assessment.outputs.dangerous_files }}
        
        This PR meets all automated quality criteria and has been approved by the AI system.
        
        ---
        *Auto-approved by GitHub Copilot AI Review System*"
        
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: üîÑ Auto-Merge if Approved
      if: steps.ai-decision.outputs.auto_approve == 'true' && steps.ai-decision.outputs.confidence == 'HIGH'
      run: |
        echo "üîÑ Auto-merging high-confidence PR..."
        
        # Wait for status checks to complete
        sleep 30
        
        # Auto-merge if all checks pass
        gh pr merge ${{ github.event.number }} --auto --squash --delete-branch
        
        echo "‚úÖ PR auto-merged successfully!"
        
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: üìä Generate AI Report
      if: always()
      run: |
        echo "üìä Generating AI analysis report..."
        
        cat > ai-report.md << EOF
        # ü§ñ AI Code Review Report
        
        **PR:** #${{ github.event.number }}
        **Date:** $(date)
        **Decision:** ${{ steps.ai-decision.outputs.auto_approve == 'true' && '‚úÖ AUTO-APPROVED' || '‚è∏Ô∏è MANUAL REVIEW' }}
        **Confidence:** ${{ steps.ai-decision.outputs.confidence }}
        
        ## üìà Scoring Details
        
        | Category | Score | Max | Status |
        |----------|-------|-----|--------|
        | Code Quality | ${{ steps.ai-assessment.outputs.score }} | 100 | ${{ steps.ai-assessment.outputs.score >= 70 && '‚úÖ' || '‚ö†Ô∏è' }} |
        | Testing | ${{ steps.automated-tests.outputs.test_score }} | 50 | ${{ steps.automated-tests.outputs.test_score >= 30 && '‚úÖ' || '‚ö†Ô∏è' }} |
        | Security | ${{ steps.security-check.outputs.security_score }} | 50 | ${{ steps.security-check.outputs.security_score >= 30 && '‚úÖ' || '‚ö†Ô∏è' }} |
        | **Total** | **${{ steps.ai-decision.outputs.total_score }}** | **200** | **${{ steps.ai-decision.outputs.total_score >= 150 && '‚úÖ' || steps.ai-decision.outputs.total_score >= 120 && '‚ö†Ô∏è' || '‚ùå' }}** |
        
        ## üîç Analysis Summary
        
        - **Files Modified:** ${{ steps.ai-assessment.outputs.file_count }}
        - **Infrastructure Changes:** ${{ steps.ai-assessment.outputs.infrastructure_files > 0 && 'Yes' || 'No' }}
        - **Documentation Updated:** ${{ steps.ai-assessment.outputs.documentation_files > 0 && 'Yes' || 'No' }}
        - **Security Risks:** ${{ steps.ai-assessment.outputs.dangerous_files > 0 && 'Detected' || 'None' }}
        
        ## üéØ AI Recommendation
        
        ${{ steps.ai-decision.outputs.auto_approve == 'true' && 'This PR demonstrates high quality and can be safely auto-approved.' || 'This PR requires manual review due to quality concerns or security considerations.' }}
        
        ---
        *Generated by GitHub Copilot AI Review System*
        EOF
        
        # Upload as artifact
        echo "üìÅ Report generated: ai-report.md"

    - name: üì¢ Notify Manual Review Required
      if: steps.ai-decision.outputs.auto_approve != 'true'
      run: |
        echo "üì¢ Notifying that manual review is required..."
        
        gh pr comment ${{ github.event.number }} --body "## ‚è∏Ô∏è Manual Review Required
        
        The AI system has determined this PR requires human review.
        
        **Score:** ${{ steps.ai-decision.outputs.total_score }}/200 (Threshold: 120)
        **Issues Detected:**
        - Quality score below threshold
        - Potentially sensitive files detected
        - Security concerns identified
        
        @aviladevs please review this PR manually.
        
        ---
        *AI Review System*"
        
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # Learning system - collect data for improving AI
  ai-learning:
    name: üß† AI Learning System
    runs-on: ubuntu-latest
    if: always()
    needs: ai-review-and-approve
    
    steps:
    - name: üìä Collect Learning Data
      run: |
        echo "üìä Collecting data for AI learning..."
        
        # This would typically send data to a learning system
        # For now, we'll just log it
        echo "PR: ${{ github.event.number }}"
        echo "Author: ${{ github.event.pull_request.user.login }}"
        echo "Auto-approved: ${{ needs.ai-review-and-approve.outputs.auto_approve }}"
        echo "Score: ${{ needs.ai-review-and-approve.outputs.total_score }}"
        
        # In a real implementation, this would:
        # 1. Send data to ML pipeline
        # 2. Update model weights
        # 3. Improve future decisions